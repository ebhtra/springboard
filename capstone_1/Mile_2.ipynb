{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1894852, 9)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "checkins = pd.read_csv('comboframe.csv', usecols=['beer_id', 'rating_user',\n",
    "                                                  'rating_global', 'user_id',\n",
    "                                                  'abv', 'brewery_name',\n",
    "                                                  'beer_style', 'beer_name',\n",
    "                                                  'checkin_id'])\n",
    "# only allow each user one rating for each beer\n",
    "checkins.drop_duplicates(subset=['beer_id', 'user_id'], inplace=True)\n",
    "print(checkins.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1529580, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the users who only checked fewer than 4 times\n",
    "checkins = checkins[checkins.user_id.map(checkins.groupby('user_id').size() > 3)]\n",
    "checkins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1257869, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the beers rated fewer than 4 times\n",
    "checkins = checkins[checkins.beer_id.map(checkins.groupby('beer_id').size() > 3)]\n",
    "checkins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1193682, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the ones with no global ratings\n",
    "checkins.dropna(subset=['rating_global'], axis=0, inplace=True)\n",
    "# and the zeros\n",
    "checkins = checkins[checkins.rating_global > 0]\n",
    "checkins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1184752, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove beers that aren't really beer, based on the alcohol content\n",
    "checkins = checkins[(checkins.abv > 0) & (checkins.abv < 20)]\n",
    "checkins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>checkin_id</th>\n",
       "      <th>beer_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>rating_user</th>\n",
       "      <th>brewery_name</th>\n",
       "      <th>beer_name</th>\n",
       "      <th>beer_style</th>\n",
       "      <th>rating_global</th>\n",
       "      <th>abv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>821797539</td>\n",
       "      <td>2095023</td>\n",
       "      <td>3340203</td>\n",
       "      <td>3.75</td>\n",
       "      <td>Stone Brewing</td>\n",
       "      <td>Stone Scorpion Bowl IPA</td>\n",
       "      <td>IPA - American</td>\n",
       "      <td>3.73789</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>818949121</td>\n",
       "      <td>1709568</td>\n",
       "      <td>3340203</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Ritual Brewing Company</td>\n",
       "      <td>Pale Ale</td>\n",
       "      <td>Pale Ale - American</td>\n",
       "      <td>3.43165</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   checkin_id  beer_id  user_id  rating_user            brewery_name  \\\n",
       "0   821797539  2095023  3340203         3.75           Stone Brewing   \n",
       "1   818949121  1709568  3340203         3.50  Ritual Brewing Company   \n",
       "\n",
       "                 beer_name           beer_style  rating_global  abv  \n",
       "0  Stone Scorpion Bowl IPA       IPA - American        3.73789  7.5  \n",
       "1                 Pale Ale  Pale Ale - American        3.43165  5.2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkins.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a mapping from user to number of ratings can be helpful in many situations\n",
    "usercounts = checkins.groupby('user_id').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_last_X(frame, countDict, X):\n",
    "    '''\n",
    "    Split the input frame into training and testing,\n",
    "    using the last X for each user as testers.\n",
    "    CountDict input has the rows per user,\n",
    "    and the frame is indexed by user.\n",
    "    Returns the train split and test split\n",
    "    '''\n",
    "    boollist = [[True] * (countDict[u] - X) + [False] * X for u in frame.index.unique()]\n",
    "    boollist = np.array([boo for lis in boollist for boo in lis])  # numpy to help with the logic\n",
    "    \n",
    "    return frame[boollist], frame[~boollist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a func to deal with ties in rankings\n",
    "def untied_rank(arr, vals):\n",
    "    '''\n",
    "    Measure how well the input vals (list or np.array) has chosen\n",
    "    the top values of input arr (np.array). \n",
    "    vals must be subset of arr.\n",
    "    1.0 is perfect, 0.0 is worst.\n",
    "    '''\n",
    "    fails = 0\n",
    "    poss_fails = 0\n",
    "    ordered = np.sort(arr)\n",
    "    if max(ordered) == min(ordered): return 0.5  # like guessing, if all equal\n",
    "    for i in range(len(vals)):\n",
    "        fails += sum(arr > vals[i])\n",
    "        arr = np.delete(arr, np.where(arr == vals[i])[0][0])\n",
    "        poss_fails += sum(ordered > ordered[i])\n",
    "    \n",
    "    return 1 - fails / poss_fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1112372 training rows, 72380 testing rows\n"
     ]
    }
   ],
   "source": [
    "# with sorted checkins for each user, we can simulate having a user's rating history when recommending\n",
    "checkins.sort_values(by=['user_id', 'checkin_id'], inplace=True)\n",
    "checkins.set_index('user_id', inplace=True)\n",
    "# gather the users who have more than 50 ratings and save their last 10 for testing\n",
    "bigs = checkins.index.map(usercounts) > 49\n",
    "smalls = checkins[~bigs]\n",
    "tr, test = split_last_X(checkins[bigs], usercounts, 10)\n",
    "train = pd.concat([smalls, tr])\n",
    "print(f'{train.shape[0]} training rows, {test.shape[0]} testing rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need the space\n",
    "checkins = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the baseline score of recommending top-rated beers, regardless of user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score picking the top 3 globally rated for 7238 \"menus\":  0.7752973184428179\n"
     ]
    }
   ],
   "source": [
    "# baseline (top picks) results/target\n",
    "top_rank_scores = []\n",
    "for u in test.index.unique():\n",
    "    utest = test.loc[u, ['rating_user','rating_global']]\n",
    "    top_rank_scores.append(untied_rank(utest.rating_user.values, \n",
    "                              utest.rating_user.values[np.argsort(utest.rating_global.values)[:-4:-1]]))\n",
    "print(f'The average score picking the top 3 globally rated for {len(top_rank_scores)} \"menus\":  {np.mean(top_rank_scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now see where collaborative filtering can get us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show how each user's ratings deviate from global ratings and from their own ratings\n",
    "train['udiff'] = train.rating_user - train.rating_global\n",
    "train['udev'] = train.udiff - train.index.map(train.groupby(train.index)['udiff'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.reset_index(inplace=True)\n",
    "udict = {uid:dict() for uid in train.user_id.unique()}\n",
    "bdict = {bid:dict() for bid in train.beer_id.unique()}\n",
    "for checkin in zip(train.user_id, train.beer_id, train.udev):\n",
    "    udict[checkin[0]][checkin[1]] = checkin[2]\n",
    "    bdict[checkin[1]][checkin[0]] = checkin[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43641/43641 [1:21:28<00:00,  8.93it/s]    \n"
     ]
    }
   ],
   "source": [
    "# in order to pickle the default dict, use this instead of a lambda:\n",
    "def dd():\n",
    "    return defaultdict(float)\n",
    "\n",
    "# calculate user/user correlation/similarity \n",
    "shared = {u: defaultdict(dd) for u in udict}\n",
    "# this may take a minute or maybe an hour, depending on memory, with the nested loops, but seems messy otherwise\n",
    "for u in tqdm(udict):\n",
    "    # loop thru all checkins by u\n",
    "    for b in udict[u]:\n",
    "        # update the similarity factors for u-v for every checkin, as would happen with new checkins\n",
    "        for v in bdict[b]:\n",
    "            suv = shared[u][v]\n",
    "            suv['count'] += 1  # increment the common u-v ratings\n",
    "            #### stats.stackexchange has these going last, but then each new sample\n",
    "            ##### updates to a diff from mean that it contributes to, which seems wrong\n",
    "            ###### (and messes up the first one, where u_bar == u[b])  ####\n",
    "            u_dev = udict[u][b] - suv['u_bar']\n",
    "            v_dev = udict[v][b] - suv['v_bar']\n",
    "            suv['numer'] += u_dev * v_dev\n",
    "            suv['denom_1'] += u_dev ** 2\n",
    "            suv['denom_2'] += v_dev ** 2\n",
    "            #####\n",
    "            suv['u_bar'] = ((suv['count']-1) * suv['u_bar'] + udict[u][b]) / suv['count']\n",
    "            suv['v_bar'] = ((suv['count']-1) * suv['v_bar'] + udict[v][b]) / suv['count']\n",
    "            \n",
    "    # remove self-edges\n",
    "    del shared[u][u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# get this thing saved, since the memory required to build it is not always available\n",
    "with open('suvdict.pkl', 'wb') as f:\n",
    "    pickle.dump(shared, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# easier just to have this dict sitting by and ready to use\n",
    "userbiasdict = dict(train.groupby('user_id').udiff.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "preds = []\n",
    "actual = []\n",
    "lam_4 = 5\n",
    "k = 2\n",
    "eps = 1\n",
    "\n",
    "for row in zip(testers.user_id[:1000], testers.beer_id[:1000], testers.rating_user[:1000]):\n",
    "    try:\n",
    "        x = row[1]\n",
    "        u = row[0]\n",
    "        baseline = beer_mu[x] + userbiasdict[u]\n",
    "        # only looking for users v who rated the beer x in question\n",
    "        uv = [v for v in bdict[x] if v in shared[u]]\n",
    "        if not uv:\n",
    "            continue\n",
    "        suv = [shared[u][v] for v in uv]\n",
    "        # first get u-v pearsons\n",
    "        ## https://stats.stackexchange.com/questions/410468/online-update-of-pearson-coefficient\n",
    "        sims = [s['numer'] / np.sqrt(s['denom_1'] * s['denom_2'])\n",
    "                    for s in suv]  \n",
    "        # then shrink in relation to number of common u-v ratings\n",
    "        sims = np.multiply(np.array(sims),\n",
    "                           np.array([s['count'] / (s['count'] + lam_4) for s in suv]))\n",
    "        most_sim = sorted(list(zip(uv, sims)), key=lambda x: abs(x[1]), reverse=True)\n",
    "        # take the k most/least similar, or all for the many short lists\n",
    "        most_sim = most_sim[:min(k, len(most_sim))]\n",
    "        tweaks = [udict[sim[0]][x] * sim[1] for sim in most_sim]\n",
    "        tweak = sum(tweaks) / (sum(abs(sim[1]) for sim in most_sim) + eps)\n",
    "        preds.append(baseline + tweak)\n",
    "        actual.append(row[2])\n",
    "    except KeyError:\n",
    "        continue\n",
    "    except ZeroDivisionError:\n",
    "        print(row)\n",
    "        continue\n",
    "\n",
    "diffs = np.array(preds) - np.array(actual)\n",
    "rmse = np.sqrt(np.dot(diffs, diffs) / len(diffs))\n",
    "rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
