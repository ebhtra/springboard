{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from surprise import SVD, KNNBasic, Reader, Dataset, accuracy\n",
    "from surprise.model_selection import cross_validate, train_test_split, KFold, GridSearchCV\n",
    "from surprise.prediction_algorithms.baseline_only import BaselineOnly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('bigframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove all but the most recent one User/Beer combo for multiple such checkins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['user_id', 'beer_id'], inplace=True)  # keep_first (default) means keep most recent checkin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296064, 28)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape  # remaining from 1.42M prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(0.25, 5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put this into surprise's format (user, beer, rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The columns must correspond to user_id, beer_id and ratings (in that order).\n",
    "data = Dataset.load_from_df(df[['user_id', 'beer_id', 'rating_user']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a cross-validation iterator\n",
    "kf = KFold(n_splits=5)\n",
    "algo = SVD()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.4726\n",
      "RMSE: 0.4729\n",
      "RMSE: 0.4733\n",
      "RMSE: 0.4758\n",
      "RMSE: 0.4729\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for trainset, testset in kf.split(data):\n",
    "    # Train the algorithm on the trainset, and predict ratings for the testset\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "\n",
    "    # Then compute RMSE\n",
    "    accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Prediction(uid=171496, iid=3034980, r_ui=4.25, est=4.4778698277391804, details={'was_impossible': False}),\n",
       " Prediction(uid=1803223, iid=1828774, r_ui=3.25, est=3.5844314206255463, details={'was_impossible': False}),\n",
       " Prediction(uid=267320, iid=399609, r_ui=4.0, est=3.981760322915016, details={'was_impossible': False}),\n",
       " Prediction(uid=2924460, iid=1187911, r_ui=4.0, est=3.6026735719768013, details={'was_impossible': False}),\n",
       " Prediction(uid=1749282, iid=2686615, r_ui=3.5, est=3.6181331817534472, details={'was_impossible': False}),\n",
       " Prediction(uid=3292172, iid=3970, r_ui=3.25, est=3.9172385217788266, details={'was_impossible': False}),\n",
       " Prediction(uid=4396115, iid=4057, r_ui=4.0, est=3.2826608617101911, details={'was_impossible': False}),\n",
       " Prediction(uid=959498, iid=1473393, r_ui=3.5, est=3.6003136089757715, details={'was_impossible': False}),\n",
       " Prediction(uid=267320, iid=9681, r_ui=4.0, est=4.3961044757993522, details={'was_impossible': False}),\n",
       " Prediction(uid=1279351, iid=647032, r_ui=3.75, est=3.5760318107450173, details={'was_impossible': False}),\n",
       " Prediction(uid=1051191, iid=2679592, r_ui=4.5, est=4.2000853709982335, details={'was_impossible': False})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does that compare to just guessing the global mean rating for every beer?  \n",
    "First we have to remove the beers that don't have global ratings.  We could  \n",
    "theoretically use the mean of all user ratings in place of the missing global  \n",
    "ratings, but for beers with few checkins, that would cheat  \n",
    "by skewing toward the rating of the user being predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_rated = df[['rating_user', 'rating_global']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296064, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_rated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating_user      5.00000\n",
       "rating_global    4.90672\n",
       "dtype: float64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_rated.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating_user      0.25\n",
       "rating_global    0.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_rated.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoops, went to the trouble of setting NaN global ratings to 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_rated = global_rated[global_rated.rating_global > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1089604, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_rated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating_user      0.25000\n",
       "rating_global    1.00423\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_rated.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51243794703341272"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the rmse using global rating as prediction\n",
    "gr = global_rated\n",
    "diffs = gr.rating_global.values - gr.rating_user.values\n",
    "sum_errs_sq = np.dot(diffs, diffs)\n",
    "rmse = np.sqrt(sum_errs_sq / len(diffs))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So at least the Surprise rmse was lower than that of (the `global_rated` subset of) the whole population.\n",
    "  \n",
    "Quick check of what pct of ratings had globals: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.07 pct of checkins had global ratings.\n"
     ]
    }
   ],
   "source": [
    "print(f'{round(len(global_rated) * 100 / len(df), 3)} pct of checkins had global ratings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>naively predict global_mean_rating</td>\n",
       "      <td>0.512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Surprise SVD with default params</td>\n",
       "      <td>0.474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               method   rmse\n",
       "0  naively predict global_mean_rating  0.512\n",
       "1    Surprise SVD with default params  0.474"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# might as well keep track of evaluation results\n",
    "methods = ['naively predict global_mean_rating']\n",
    "rmses = [0.512]\n",
    "results = pd.DataFrame({'method': methods,\n",
    "                        'rmse': rmses})\n",
    "\n",
    "def add_results_row(meth_name, rmse_result):\n",
    "    methods.append(meth_name)\n",
    "    rmses.append(rmse_result)\n",
    "    results = pd.DataFrame({'method': methods,\n",
    "                            'rmse': rmses})\n",
    "    return results\n",
    "\n",
    "# make sure function works\n",
    "results = add_results_row('Surprise SVD with default params', 0.474)\n",
    "results.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>naively predict global_mean_rating</td>\n",
       "      <td>0.512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Surprise SVD with default params</td>\n",
       "      <td>0.474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>naive mean plus userbias</td>\n",
       "      <td>0.456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>naive mean plus smart userbias</td>\n",
       "      <td>0.434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVD lr=.02 epochs=20 reg=0.1</td>\n",
       "      <td>0.466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               method   rmse\n",
       "0  naively predict global_mean_rating  0.512\n",
       "1    Surprise SVD with default params  0.474\n",
       "2            naive mean plus userbias  0.456\n",
       "3      naive mean plus smart userbias  0.434\n",
       "4        SVD lr=.02 epochs=20 reg=0.1  0.466"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append result rows here and check tail to make sure all good\n",
    "results = add_results_row('SVD lr=.02 epochs=20 reg=0.1', 0.466)\n",
    "results.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just make sure the error for the testset alone was about the same (.512) as for whole population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "beer_id\n",
       "1     3.27892\n",
       "2     3.49213\n",
       "8     3.81242\n",
       "10    3.38719\n",
       "14    3.77430\n",
       "Name: rating_global, dtype: float64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a dictionary to map all beer_id's to global ratings, where available\n",
    "bidrates = df[df.rating_global > 0].groupby('beer_id').rating_global.mean()\n",
    "bidrates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbr = dict(bidrates)  # map beer_id's to glob_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51115640157523945"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now line up all the beers in just the test set,\n",
    "#   filter out the ones with no global, and put them\n",
    "#   alongside all their ratings.\n",
    "\n",
    "globrates = pd.Series([dbr.get(triple[1], 0) for triple in testset])\n",
    "userrates = pd.Series([triple[2] for triple in testset])\n",
    "userrates = userrates[globrates > 0]\n",
    "globrates = globrates[globrates > 0]\n",
    "# calc rmse\n",
    "diffs = userrates.values - globrates.values\n",
    "sum_errs_sq = np.dot(diffs, diffs)\n",
    "rmse = np.sqrt(sum_errs_sq / len(diffs))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's factor in the user's rating generosity, which should help the naive guess a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use groupby to get each rater's mean, \n",
    "# then use that mapping to add a new column to the end of the df for each checkin\n",
    "userbias = dict(df.groupby('user_id').rating_user.mean())\n",
    "df['userbias'] = df.user_id.map(lambda u: userbias[u]) - df.rating_user.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45625249039480498"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_df = df[['rating_user', 'rating_global', 'userbias']]\n",
    "bias_df = bias_df[bias_df.rating_global > 0]\n",
    "# add the user's generosity vs the mean\n",
    "diffs = bias_df.rating_global.values + bias_df.userbias.values - bias_df.rating_user.values\n",
    "sum_errs_sq = np.dot(diffs, diffs)\n",
    "rmse = np.sqrt(sum_errs_sq / len(diffs))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That helped a lot.  A single 'feature' is already better than the Surprise SVD model.\n",
    "But the calculated userbias is simply how a user rates vis-a-vis the average rating,\n",
    "and doesn't take into account the fact that certain raters rate higher-rated beers.\n",
    "If user A lives where beers globally rated 4.5 stars are available, and gives them 4.25 stars,\n",
    "while user B rates average beers 4.0 where others rated them 3.75, why does user A get\n",
    "an extra quarter star added to his predictions compared to user B?  I.e. our generosity measure\n",
    "should maybe account for which beers are being rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each user, find the difference between their ratings for their beers vs. the world's ratings for same ones\n",
    "userbeers = dict(userrates.rating_user.mean() - userrates.rating_global.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['userbeerbias'] = df[df.rating_global > 0].user_id.map(lambda u: userbeers[u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43369005337892269"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_df = df[['rating_user', 'rating_global', 'userbeerbias']]\n",
    "bias_df = bias_df[bias_df.rating_global > 0]\n",
    "# add the user's generosity vs the mean\n",
    "diffs = bias_df.rating_global.values + bias_df.userbeerbias.values - bias_df.rating_user.values\n",
    "sum_errs_sq = np.dot(diffs, diffs)\n",
    "rmse = np.sqrt(sum_errs_sq / len(diffs))\n",
    "rmse "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That does better.  Let's see if it aligns with Surprise's BaselineOnly model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline = BaselineOnly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.baseline_only.BaselineOnly at 0x172b2f4a8>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.4688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46881078934132836"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = baseline.test(testset)\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmmmmm, thought that should be 0.434....guess surprise is using estimations where i used actual global ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Surprise can't keep up with simple baseline predictions, at least while using default parameters, maybe it's time for some GridSearching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_epochs': [5, 10, 15], 'lr_all': [0.001, 0.002, 0.005, 0.01],\n",
    "              'reg_all': [0.2, 0.5, 1.0, 2.0]}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.470300063255\n",
      "{'n_epochs': 15, 'lr_all': 0.01, 'reg_all': 0.2}\n"
     ]
    }
   ],
   "source": [
    "gs.fit(data)\n",
    "\n",
    "# best RMSE score\n",
    "print(gs.best_score['rmse'])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much improvement there from the default params (.474 improved to .470), but since the best  \n",
    "params out of the grid were all at the end of the param range (15 epochs was the most in the grid,  \n",
    "0.01 was the highest learning rate, and 0.2 was the lowest regularization), let's try fitting  \n",
    "with the next step out in the directions of those params.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "algo = SVD(lr_all=0.02, n_epochs=20, reg_all=0.1, verbose=True)\n",
    "# sample random trainset and testset\n",
    "# test set is made of 25% of the ratings.\n",
    "trainset, testset = train_test_split(data, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "RMSE: 0.4664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46640651047237058"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better than 0.470, so keep trying, with higher step rate, and lower regularization, until things reverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "RMSE: 0.4683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46830288741631865"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = SVD(lr_all=0.03, n_epochs=20, reg_all=0.05, verbose=True)\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That didn't take long.  So store the best params from previous fit into the results table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python3.6/site-packages/surprise/prediction_algorithms/algo_base.py:248: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sim = construction_func[name](*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done computing similarity matrix.\n",
      "RMSE: 0.5886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.58863602712876439"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_options = {'name': 'pearson'}\n",
    "algo = KNNBasic(sim_options=sim_options)\n",
    "algo.fit(trainset)\n",
    "preds = algo.test(testset)\n",
    "accuracy.rmse(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do individual user tastes determine which features correlate with ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['checkin_id', 'beer_id', 'user_id', 'rating_user', 'brewery_name',\n",
       "       'beer_name', 'beer_style', 'brewery_id', 'brewery_type',\n",
       "       'brewery_country', 'brewery_city', 'brewery_state', 'brewery_lat',\n",
       "       'brewery_lon', 'venue_lat', 'venue_lon', 'venue_city', 'venue_country',\n",
       "       'venue_state', 'venue_cat', 'venue_id', 'checkin_comment', 'venue_type',\n",
       "       'rating_global', 'beer_description', 'abv', 'date', 'userbias',\n",
       "       'userbeerbias'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296064, 29)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==========================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
